{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69a9a1b",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    color: blue; /* Text color */\n",
    "    background-color: #f0f0f0; /* Light grey background */\n",
    "    font-family: 'Pacifico', cursive; /* Stylish font */\n",
    "    font-size: 40px; /* Font size */\n",
    "    font-weight: bold; /* Bold text */\n",
    "    padding: 20px; /* Padding around the text */\n",
    "    border: 2px solid #ccc; /* Border with a light grey color */\n",
    "    border-radius: 10px; /* Rounded corners */\n",
    "    text-align: center; /* Centered text */\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Subtle shadow */\n",
    "    position: relative; /* Required for pseudo-elements positioning */\n",
    "    display: inline-block; /* Inline-block for proper spacing */\n",
    "\">\n",
    "    <span style=\"\n",
    "        position: absolute; /* Absolute positioning */\n",
    "        left: -40px; /* Positioning arrow on the left */\n",
    "        top: 50%; /* Vertically center */\n",
    "        transform: translateY(-50%); /* Adjust vertical alignment */\n",
    "        font-size: 30px; /* Arrow size */\n",
    "        color: red; /* Arrow color */\n",
    "    \">➤</span>\n",
    "    ML Modeling\n",
    "    <span style=\"\n",
    "        position: absolute; /* Absolute positioning */\n",
    "        right: -40px; /* Positioning arrow on the right */\n",
    "        top: 50%; /* Vertically center */\n",
    "        transform: translateY(-50%); /* Adjust vertical alignment */\n",
    "        font-size: 30px; /* Arrow size */\n",
    "        color: red; /* Arrow color */\n",
    "    \">➤</span>\n",
    "</div>\n",
    "\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Pacifico&display=swap\" rel=\"stylesheet\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "b5be8d3a-c92b-4fa3-93b6-7f0b1ed7efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e9d5c-d291-4eb4-9e58-4cb9d0d8bf83",
   "metadata": {},
   "source": [
    "# Tuning Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9265255c-bc6e-4833-9f6f-038398bdd97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "def get_X_Y (exception_countries = []):\n",
    "    df = pd.read_csv(\"../Data/Raw/new_approach/Weekly_Covid_Data.csv\")\n",
    "\n",
    "    # delete called countries:\n",
    "    for iso_code in exception_countries:\n",
    "        df = df[df[\"iso_code\"] != iso_code]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    country_num = int(len(df[\"iso_code\"].unique()))\n",
    "    \n",
    "    df.drop([\"iso_code\", \"week_no\"], axis=1, inplace=True)\n",
    "    #df.drop(columns=df.columns[0:1], axis=1, inplace=True)\n",
    "    pop = df.iloc[0][\"population\"]\n",
    "    last_country = df.iloc[country_num*176 - 1]\n",
    "    print(last_country)\n",
    "    cnt = df[\"population\"].value_counts()\n",
    "    \n",
    "    # now we want to join datas together\n",
    "    prediction_days_count = 2\n",
    "    features_list = []\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    \n",
    "    for i in range(country_num):  # 234 is number of countries\n",
    "        for j in range(176):  # 176 is number of weeks we have\n",
    "            row_list = df.loc[i * 176 + j, :].values.flatten().tolist()\n",
    "            features_list.append(row_list)\n",
    "    \n",
    "    for i in range((len(features_list) - prediction_days_count) + 1):\n",
    "        fl = features_list[i]\n",
    "        fl_len = len(features_list[i])\n",
    "        this_population = features_list[i][fl_len - 1]\n",
    "        for j in range(1, prediction_days_count):\n",
    "            if this_population == features_list[i + j][fl_len - 1]:\n",
    "                fl.extend(features_list[i + j])\n",
    "            else:\n",
    "                this_population = -1\n",
    "                break\n",
    "        if this_population != -1:\n",
    "            X_list.append(fl)\n",
    "            Y_list.append(features_list[(i + prediction_days_count) - 1][0])\n",
    "    \n",
    "    return X_list, Y_list, country_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d96fed-8328-4d96-b3b2-6f097bad7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class Moldels_Tuning (object):\n",
    "    def __init__ (self, X_list, Y_list, country_num, prediction_days_count, \\\n",
    "                  batch_count, batch_length, val_size):\n",
    "        x_df = pd.DataFrame(X_list)\n",
    "        self._min_max_scaler_X = preprocessing.MinMaxScaler()\n",
    "        x_scaled = self._min_max_scaler_X.fit_transform(x_df)\n",
    "        \n",
    "        y_df = pd.DataFrame(Y_list)\n",
    "        self._min_max_scaler_Y = preprocessing.MinMaxScaler()\n",
    "        y_scaled = self._min_max_scaler_Y.fit_transform(y_df)\n",
    "        \n",
    "        X_scaled_list = x_scaled.tolist()\n",
    "        Y_scaled_list = y_scaled.tolist()\n",
    "        \n",
    "        self._X_train, self._X_test, self._Y_train, self._Y_test = self.__train_test_splitter (X_scaled_list, Y_scaled_list, country_num,prediction_days_count, batch_count, batch_length)\n",
    "        self.X_train = self._X_train\n",
    "        self.X_test = self._X_test\n",
    "        self.Y_train = self._Y_train\n",
    "        self.Y_test = self._Y_test\n",
    "        self._val_size = val_size\n",
    "\n",
    "        # initial values\n",
    "        # svm\n",
    "        self.svm_rbf_min_loss_params_ = {\"C\": None,\n",
    "                           \"epsilon\": None,\n",
    "                           \"gamma\": None,\n",
    "                           \"degree\": None,\n",
    "                           \"coef0\": None,\n",
    "                           \"min_loss_value\": None}\n",
    "        self.svm_rbf_losses_ = []\n",
    "        self.svm_linear_min_loss_params_ = {\"C\": None,\n",
    "                           \"epsilon\": None,\n",
    "                           \"gamma\": None,\n",
    "                           \"degree\": None,\n",
    "                           \"coef0\": None,\n",
    "                           \"min_loss_value\": None}\n",
    "        self.svm_linear_losses_ = []\n",
    "        self.svm_poly_min_loss_params_ = {\"C\": None,\n",
    "                           \"epsilon\": None,\n",
    "                           \"gamma\": None,\n",
    "                           \"degree\": None,\n",
    "                           \"coef0\": None,\n",
    "                           \"min_loss_value\": None}\n",
    "        self.svm_poly_losses_ = []\n",
    "\n",
    "        # KNN\n",
    "        self.KNN_min_loss_params_ = {\"n_neighbor\": None,\n",
    "                          \"p\": None,\n",
    "                          \"weight\": None,\n",
    "                          \"algorithm\": None,\n",
    "                          \"leaf_size\": None,\n",
    "                          \"min_loss_value\": None}\n",
    "        self.KNN_losses_ = []\n",
    "        # Ada Bosst\n",
    "        self.Ada_Boost_min_loss_params_ = {\"n_estimator\" : None,\n",
    "                            \"learning_rate\" : None,\n",
    "                            \"random_state\" : None,\n",
    "                            \"estimator\" : None,\n",
    "                            \"min_loss_value\" : None}\n",
    "        self.Ada_Bosst_losses_ = []\n",
    "\n",
    "        # Poly\n",
    "        self.Poly_min_loss_params_ = {\"degree\": None,\n",
    "                              \"interaction_only\": None,\n",
    "                              \"include_bias\": None,\n",
    "                              \"min_loss_value\": None}\n",
    "        self.Poly_losses_ = []\n",
    "    \n",
    "        # Random Forrest:\n",
    "        self.random_forrest_min_loss_params_ = {\"n_estimator\" : None,\n",
    "                                \"max_depth\" : None,\n",
    "                                \"min_samples_split\" : None,\n",
    "                                \"min_samples_leaf\" : None,\n",
    "                                \"bootstrap\" : None,\n",
    "                                \"random_state\" : None,\n",
    "                                \"max_features\" : None,\n",
    "                                \"min_loss_value\" : None}\n",
    "        self.random_forrset_losses_ = []\n",
    "         # Stacking:\n",
    "        self.Stacking_min_loss_params_ = {\"estimators\": None,\n",
    "                                           \"final_estimator\": None,\n",
    "                                           \"min_loss_value\": float('inf')}\n",
    "        \n",
    "    \n",
    "    def __train_test_splitter(self, X, Y, country_num,prediction_days_count, batch_count, batch_length):\n",
    "        xlen = len(X)\n",
    "        xs_count_for_each_country = int(xlen / country_num)\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        steps = math.floor((xs_count_for_each_country - (batch_count * batch_length)) / (batch_count + 1)) \n",
    "        i = 0\n",
    "        for i in range(country_num):\n",
    "            j = 0\n",
    "            for j in range((i * xs_count_for_each_country), (i * xs_count_for_each_country) + steps):\n",
    "                X_train.append(X[j])\n",
    "                Y_train.append(Y[j])\n",
    "            while(j + batch_length + steps < (i+1) * xs_count_for_each_country):\n",
    "                for k in range(j + prediction_days_count, j + batch_length - prediction_days_count):\n",
    "                    X_test.append(X[k])\n",
    "                    Y_test.append(Y[k])\n",
    "                j += batch_length\n",
    "                for k in range(j, j + steps):\n",
    "                    X_train.append(X[k])\n",
    "                    Y_train.append(Y[k])\n",
    "                j += steps + 1\n",
    "                while j < (i+1) * xs_count_for_each_country:\n",
    "                    X_train.append(X[j])\n",
    "                    Y_train.append(Y[j])\n",
    "                    j += 1\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "\n",
    "    def __train_val_spliter (self, seed=0):\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(self._X_train, self._Y_train, \\\n",
    "                                                          test_size=self._val_size, random_state=seed)\n",
    "        X_tr = np.array(X_train)\n",
    "        y_tr = np.array(Y_train).reshape(len(Y_train))\n",
    "        X_val = np.array(X_val)\n",
    "        y_val = np.array(Y_val).reshape(len(Y_val))\n",
    "        return X_tr, y_tr, X_val, y_val\n",
    "        \n",
    "    def __loss_function (self, y_test, y_pred):\n",
    "        return np.sqrt((np.sum(np.power(y_test - y_pred, 2)))/len(y_test))\n",
    "    \n",
    "    def __rescale_data(self, data):\n",
    "        return self._min_max_scaler_Y.inverse_transform(np.array(data).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    def __evaluate_model(self, model, X_tr, y_tr, X_val, y_val):\n",
    "        y_pred_scaled = model.fit(X_tr, y_tr).predict(X_val)\n",
    "        y_pred_rescaled = self.__rescale_data(y_pred_scaled)\n",
    "        y_val_rescaled = self.__rescale_data(y_val)\n",
    "    \n",
    "        # deviding by 10:\n",
    "        y_pred_rescaled /= 10\n",
    "        y_val_rescaled /= 10\n",
    "        \n",
    "        return self.__loss_function (y_val_rescaled, y_pred_rescaled)\n",
    "        \n",
    "    def __evaluate_model_test (self, model, X_te, y_te):\n",
    "        y_pred_scaled = model.predict(X_te)\n",
    "        y_pred_rescaled = self.__rescale_data(y_pred_scaled)\n",
    "        y_test_rescaled = self.__rescale_data(y_te)\n",
    "    \n",
    "        # deviding by 10:\n",
    "        y_pred_rescaled /= 10\n",
    "        y_test_rescaled /= 10\n",
    "        \n",
    "        return self.__loss_function (y_test_rescaled, y_pred_rescaled)\n",
    "    \n",
    "    def svm_tuning (self, kernel,\n",
    "                    Cs,\n",
    "                    epsilons,\n",
    "                    gammas = [None],\n",
    "                    degrees = [None],\n",
    "                    coef0s = [None]):\n",
    "        seed = 0\n",
    "        min_loss_params = {\"C\": None,\n",
    "                           \"epsilon\": None,\n",
    "                           \"gamma\": None,\n",
    "                           \"degree\": None,\n",
    "                           \"coef0\": None,\n",
    "                           \"min_loss_value\": None}\n",
    "        losses = []\n",
    "        for C in Cs:\n",
    "            for epsilon in epsilons:\n",
    "                for gamma in gammas:    \n",
    "                    for degree in degrees:\n",
    "                        for coef0 in coef0s:\n",
    "                            if kernel == \"rbf\":\n",
    "                                svr = SVR (kernel = kernel, C=C, gamma = gamma, epsilon = epsilon)\n",
    "                            elif kernel == \"linear\":\n",
    "                                svr = SVR (kernel = kernel, C=C, epsilon = epsilon)\n",
    "                            elif kernel == \"poly\":\n",
    "                                svr = SVR (kernel = kernel, C=C, gamma = gamma,\n",
    "                                           epsilon = epsilon, degree=degree, coef0=coef0)\n",
    "                            else:\n",
    "                                print (\"please use valid kernel.\")\n",
    "                                return\n",
    "                            X_tr, y_tr, X_val, y_val = \\\n",
    "                                    self.__train_val_spliter(seed=seed)\n",
    "                            loss = self.__evaluate_model (model=svr,\n",
    "                                                   X_tr=X_tr,\n",
    "                                                   y_tr=y_tr,\n",
    "                                                   X_val=X_val,\n",
    "                                                   y_val=y_val)\n",
    "                            if min_loss_params[\"min_loss_value\"] == None \\\n",
    "                                or loss < min_loss_params[\"min_loss_value\"]:\n",
    "                                min_loss_params[\"C\"] = C\n",
    "                                min_loss_params[\"epsilon\"] = epsilon\n",
    "                                min_loss_params[\"gamma\"] = gamma\n",
    "                                min_loss_params[\"degree\"] = degree\n",
    "                                min_loss_params[\"coef0\"] = coef0\n",
    "                                min_loss_params[\"min_loss_value\"] = loss\n",
    "                            losses.append(loss)\n",
    "                            seed += 1\n",
    "        if kernel == \"rbf\":\n",
    "            self.svm_rbf_min_loss_params_ = min_loss_params\n",
    "            self.svm_rbf_losses_ = losses\n",
    "        elif kernel == \"linear\":\n",
    "            self.svm_linear_min_loss_params_ = min_loss_params\n",
    "            self.svm_linear_losses_ = losses\n",
    "        elif kernel == \"poly\":\n",
    "            self.svm_poly_min_loss_params_ = min_loss_params\n",
    "            self.svm_poly_losses_ = losses\n",
    "        return\n",
    "    \n",
    "    def svm_score (kernel = \"rbf\"):\n",
    "        if kernel == \"rbf\":\n",
    "            if self.svm_rbf_min_loss_params_[\"min_loss_value\"] != None:\n",
    "                C = self.svm_rbf_min_loss_params_[\"C\"]\n",
    "                epsilon = self.svm_rbf_min_loss_params_[\"epsilon\"]\n",
    "                gamma = self.svm_rbf_min_loss_params_[\"gamma\"]\n",
    "                coef0 = self.svm_rbf_min_loss_params_[\"coef0\"]\n",
    "                svr = SVR (kernel = kernel, C=C, gamma = gamma, epsilon = epsilon)\n",
    "                loss_value = self.__evaluate_model_test (model=svr, X_te=self._X_test,\n",
    "                                            y_te=self._Y_test)\n",
    "                return loss_value\n",
    "\n",
    "        elif kernel == \"linear\":\n",
    "            if self.svm_linear_min_loss_params_[\"min_loss_value\"] != None:\n",
    "                C = self.svm_linear_min_loss_params_[\"C\"]\n",
    "                epsilon = self.svm_linear_min_loss_params_[\"epsilon\"]\n",
    "                svr = SVR (kernel = kernel, C=C, epsilon = epsilon)\n",
    "                loss_value = self.__evaluate_model_test (model=svr, X_te=self._X_test,\n",
    "                                            y_te=self._Y_test)\n",
    "                return loss_value\n",
    "        elif kernel == \"poly\":\n",
    "            if self.svm_poly_min_loss_params_[\"min_loss_value\"] != None:\n",
    "                C = self.svm_poly_min_loss_params_[\"C\"]\n",
    "                epsilon = self.svm_poly_min_loss_params_[\"epsilon\"]\n",
    "                degree = self.svm_poly_min_loss_params_[\"degree\"]\n",
    "                gamma = self.svm_poly_min_loss_params_[\"gamma\"]\n",
    "                coef0 = self.svm_poly_min_loss_params_[\"coef0\"]\n",
    "                svr = SVR (kernel = kernel, C=C, gamma = gamma,\n",
    "                                           epsilon = epsilon, degree=degree, coef0=coef0)\n",
    "                loss_value = self.__evaluate_model_test (model=svr, X_te=self._X_test,\n",
    "                                            y_te=self._Y_test)\n",
    "                return loss_value\n",
    "        return\n",
    "    \n",
    "    def KNN_tuning (self, n_neighbors,\n",
    "                    ps=[1,2],\n",
    "                    weights=['uniform', 'distance'],\n",
    "                    algorithms=['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                    leaf_sizes=[30]):\n",
    "        seed = 0\n",
    "        min_loss_params= {\"n_neighbor\": None,\n",
    "                          \"p\": None,\n",
    "                          \"weight\": None,\n",
    "                          \"algorithm\": None,\n",
    "                          \"leaf_size\": None,\n",
    "                          \"min_loss_value\": None}\n",
    "        losses = []\n",
    "        for n_neighbor in n_neighbors:\n",
    "            for p in ps:\n",
    "                for weight in weights:\n",
    "                    for algorithm in algorithms:\n",
    "                        for leaf_size in leaf_sizes:\n",
    "                            knn = KNeighborsRegressor(n_neighbors=n_neighbor,\n",
    "                                                      p=p,\n",
    "                                                      weights=weight,\n",
    "                                                      algorithm=algorithm,\n",
    "                                                      leaf_size=leaf_size)\n",
    "                            X_tr, y_tr, X_val, y_val = \\\n",
    "                                    self.__train_val_spliter(seed=seed)\n",
    "                            loss = self.__evaluate_model (model=knn,\n",
    "                                                   X_tr=X_tr,\n",
    "                                                   y_tr=y_tr,\n",
    "                                                   X_val=X_val,\n",
    "                                                   y_val=y_val)\n",
    "                            if min_loss_params[\"min_loss_value\"] == None \\\n",
    "                                or loss < min_loss_params[\"min_loss_value\"]:\n",
    "                                min_loss_params[\"n_neighbor\"] = n_neighbor\n",
    "                                min_loss_params[\"p\"] = p\n",
    "                                min_loss_params[\"weight\"] = weight\n",
    "                                min_loss_params[\"algorithm\"] = algorithm\n",
    "                                min_loss_params[\"leaf_size\"] = leaf_size\n",
    "                                min_loss_params[\"min_loss_value\"] = loss\n",
    "                            losses.append(loss)\n",
    "                            seed += 1\n",
    "        self.KNN_min_loss_params_ = min_loss_params\n",
    "        self.KNN_losses_ = losses\n",
    "        return\n",
    "\n",
    "    def KNN_score ():\n",
    "        if self.KNN_min_loss_params_[\"min_loss_value\"] != None:\n",
    "            n_neighbor = self.KNN_min_loss_params_[\"n_neighbor\"]\n",
    "            p = self.KNN_min_loss_params_[\"p\"]\n",
    "            weight = self.KNN_min_loss_params_[\"weight\"]\n",
    "            algorithm = self.KNN_min_loss_params_[\"algorithm\"]\n",
    "            leaf_size = self.KNN_min_loss_params_[\"leaf_size\"]\n",
    "            knn = KNeighborsRegressor(n_neighbors=n_neighbor,\n",
    "                                                      p=p,\n",
    "                                                      weights=weight,\n",
    "                                                      algorithm=algorithm,\n",
    "                                                      leaf_size=leaf_size)\n",
    "            loss_value = self.__evaluate_model_test (model=knn, X_te=self._X_test,\n",
    "                                        y_te=self._Y_test)\n",
    "            return loss_value\n",
    "        return\n",
    "            \n",
    "    def Poly_tuning (self, degrees,\n",
    "                     interaction_onlies = [True, False],\n",
    "                     include_biases = [True, False]):\n",
    "        seed = 0\n",
    "        min_loss_params= {\"degree\": None,\n",
    "                          \"interaction_only\": None,\n",
    "                          \"include_bias\": None,\n",
    "                          \"min_loss_value\": None}\n",
    "        losses = []\n",
    "        for degree in degrees:\n",
    "            for interaction_only in interaction_onlies:\n",
    "                for include_bias in include_biases:\n",
    "                    poly = PolynomialFeatures(degree=degree,\n",
    "                                              interaction_only=interaction_only,\n",
    "                                              include_bias=include_bias)\n",
    "                    X_tr, y_tr, X_val, y_val = \\\n",
    "                                    self.__train_val_spliter(seed=seed)\n",
    "                    X_poly_tr = poly.fit_transform(X_tr)\n",
    "                    X_poly_val = poly.transform(X_val)\n",
    "\n",
    "                    linear_model = LinearSVR(loss='squared_epsilon_insensitive',\n",
    "                                             dual=False)\n",
    "                    loss = self.__evaluate_model (model=linear_model,\n",
    "                                           X_tr=X_poly_tr,\n",
    "                                           y_tr=y_tr,\n",
    "                                           X_val=X_poly_val,\n",
    "                                           y_val=y_val)\n",
    "                    if min_loss_params[\"min_loss_value\"] == None \\\n",
    "                                or loss < min_loss_params[\"min_loss_value\"]:\n",
    "                        min_loss_params[\"degree\"] = degree\n",
    "                        min_loss_params[\"interaction_only\"] = interaction_only\n",
    "                        min_loss_params[\"include_bias\"] = include_bias\n",
    "                        min_loss_params[\"min_loss_value\"] = loss\n",
    "                    losses.append(loss)\n",
    "                    seed += 1\n",
    "        self.Poly_min_loss_params_ = min_loss_params\n",
    "        self.Poly_losses_ = losses\n",
    "        return\n",
    "        \n",
    "    def Poly_score():\n",
    "        if self.Poly_min_loss_params_[\"min_loss_value\"] != None:\n",
    "            degree = self.Poly_min_loss_params_[\"degree\"]\n",
    "            interaction_only = self.Poly_min_loss_params_[\"interaction_only\"]\n",
    "            include_bias = self.Poly_min_loss_params_[\"include_bias\"]\n",
    "            poly = PolynomialFeatures(degree=degree,\n",
    "                                              interaction_only=interaction_only,\n",
    "                                              include_bias=include_bias)\n",
    "            \n",
    "            X_poly_te = poly.fit_transform(self._X_test)\n",
    "\n",
    "            linear_model = LinearSVR(loss='squared_epsilon_insensitive',\n",
    "                                     dual=False)\n",
    "            loss_value = self.__evaluate_model_test (model=linear_model, X_te=X_poly_te,\n",
    "                                        y_te=self._Y_test)\n",
    "            return loss_value\n",
    "        return\n",
    "    \n",
    "    def Ada_Boost_tuning (self, n_estimators,\n",
    "                          learning_rates,\n",
    "                          random_states = [0],\n",
    "                          base_estimators = [None]):\n",
    "        seed = 0\n",
    "        min_loss_params = {\"n_estimator\" : None,\n",
    "                            \"learning_rate\" : None,\n",
    "                            \"random_state\" : None,\n",
    "                            \"estimator\" : None,\n",
    "                            \"min_loss_value\" : None}\n",
    "        losses = []\n",
    "        for n_estimator in n_estimators:\n",
    "            for random_state in random_states:\n",
    "                for learning_rate in learning_rates:\n",
    "                    for base_estimator in base_estimators:\n",
    "                        ada_boost_regr = AdaBoostRegressor(random_state=random_state,\n",
    "                                                           n_estimators=n_estimator,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           estimator=base_estimator)\n",
    "                        X_tr, y_tr, X_val, y_val = \\\n",
    "                                        self.__train_val_spliter(seed=seed)\n",
    "    \n",
    "                        loss = self.__evaluate_model (model=ada_boost_regr,\n",
    "                                               X_tr=X_tr,\n",
    "                                               y_tr=y_tr,\n",
    "                                               X_val=X_val,\n",
    "                                               y_val=y_val)\n",
    "                        if min_loss_params[\"min_loss_value\"] == None \\\n",
    "                                    or loss < min_loss_params[\"min_loss_value\"]:\n",
    "                            min_loss_params[\"n_estimator\"] = n_estimator\n",
    "                            min_loss_params[\"learning_rate\"] = learning_rate\n",
    "                            min_loss_params[\"random_state\"] = random_state\n",
    "                            min_loss_params[\"estimator\"] = base_estimator\n",
    "                            min_loss_params[\"min_loss_value\"] = loss\n",
    "                        losses.append(loss)\n",
    "                        seed += 1\n",
    "        \n",
    "        self.Ada_Boost_min_loss_params_ = min_loss_params\n",
    "        self.Ada_Bosst_losses_ = losses\n",
    "        return\n",
    "\n",
    "    def Ada_Boost_score ():\n",
    "        if self.Ada_Boost_min_loss_params_[\"min_loss_value\"] != None:\n",
    "            n_estimator = self.Ada_Boost_min_loss_params_[\"n_estimator\"]\n",
    "            learning_rate = self.Ada_Boost_min_loss_params_[\"learning_rate\"]\n",
    "            random_state = self.Ada_Boost_min_loss_params_[\"random_state\"]\n",
    "            estimator = self.Ada_Boost_min_loss_params_[\"estimator\"]\n",
    "            ada_boost_regr = AdaBoostRegressor(random_state=random_state,\n",
    "                                                           n_estimators=n_estimator,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           estimator=base_estimator)\n",
    "            loss_value = self.__evaluate_model_test (model=ada_boost_regr,\n",
    "                                                     X_te=self._X_test,\n",
    "                                                     y_te=self._Y_test)\n",
    "            return loss_value\n",
    "        return\n",
    "    \n",
    "    def random_forrest_tuning (self, n_estimators,\n",
    "                               max_features = [1.0],\n",
    "                               max_depths = [None],\n",
    "                               min_samples_splits = [2],\n",
    "                               min_samples_leaves = [1],\n",
    "                               bootstraps = [True, False],\n",
    "                               random_states = [0]):\n",
    "        seed = 0\n",
    "        min_loss_params = {\"n_estimator\" : None,\n",
    "                            \"max_depth\" : None,\n",
    "                            \"min_samples_split\" : None,\n",
    "                            \"min_samples_leaf\" : None,\n",
    "                            \"bootstrap\" : None,\n",
    "                            \"random_state\" : None,\n",
    "                            \"max_features\" : None,\n",
    "                            \"min_loss_value\" : None}\n",
    "        losses = []\n",
    "        for n_estimator in n_estimators:\n",
    "            for max_feature in max_features:\n",
    "                for max_depth in max_depths:\n",
    "                    for min_samples_leaf in min_samples_leaves:\n",
    "                        for bootstrap in bootstraps:\n",
    "                            for random_state in random_states:\n",
    "                                for min_samples_split in min_samples_splits:\n",
    "                                    random_forrest_regr =  RandomForestRegressor(n_estimators=n_estimator,\n",
    "                                                                max_depth=max_depth,\n",
    "                                                                random_state=random_state,\n",
    "                                                                max_features=max_feature,\n",
    "                                                                min_samples_leaf=min_samples_leaf,\n",
    "                                                                min_samples_split=min_samples_split,\n",
    "                                                                bootstrap=bootstrap)\n",
    "                                    X_tr, y_tr, X_val, y_val = \\\n",
    "                                                    self.__train_val_spliter(seed=seed)\n",
    "                \n",
    "                                    loss = self.__evaluate_model (model=random_forrest_regr,\n",
    "                                                           X_tr=X_tr,\n",
    "                                                           y_tr=y_tr,\n",
    "                                                           X_val=X_val,\n",
    "                                                           y_val=y_val)\n",
    "                                    if min_loss_params[\"min_loss_value\"] == None \\\n",
    "                                                or loss < min_loss_params[\"min_loss_value\"]:\n",
    "                                        min_loss_params[\"n_estimator\"] = n_estimator\n",
    "                                        min_loss_params[\"max_depth\"] = max_depth\n",
    "                                        min_loss_params[\"min_samples_split\"] = min_samples_split\n",
    "                                        min_loss_params[\"min_samples_leaf\"] = min_samples_leaf\n",
    "                                        min_loss_params[\"bootstrap\"] = bootstrap\n",
    "                                        min_loss_params[\"random_state\"] = random_state\n",
    "                                        min_loss_params[\"max_features\"] = max_feature\n",
    "                                        min_loss_params[\"min_loss_value\"] = loss\n",
    "                                    losses.append(loss)\n",
    "                                    seed += 1          \n",
    "        self.random_forrest_min_loss_params_ = min_loss_params\n",
    "        self.random_forrset_losses_ = losses\n",
    "        return\n",
    "\n",
    "    def rendom_forrest_score ():\n",
    "        if self.random_forrest_min_loss_params_[\"min_loss_value\"] != None:\n",
    "            n_estimator = self.random_forrest_min_loss_params_[\"n_estimator\"]\n",
    "            max_depth = self.random_forrest_min_loss_params_[\"max_depth\"]\n",
    "            min_samples_split = self.random_forrest_min_loss_params_[\"min_samples_split\"]\n",
    "            min_samples_leaf = self.random_forrest_min_loss_params_[\"min_samples_leaf\"]\n",
    "            bootstrap = self.random_forrest_min_loss_params_[\"bootstrap\"]\n",
    "            random_state = self.random_forrest_min_loss_params_[\"random_state\"]\n",
    "            max_features = self.random_forrest_min_loss_params_[\"max_features\"]\n",
    "            random_forrest_regr =  RandomForestRegressor(n_estimators=n_estimator,\n",
    "                                                                max_depth=max_depth,\n",
    "                                                                random_state=random_state,\n",
    "                                                                max_features=max_feature,\n",
    "                                                                min_samples_leaf=min_samples_leaf,\n",
    "                                                                min_samples_split=min_samples_split,\n",
    "                                                                bootstrap=bootstrap)\n",
    "            loss_value = self.__evaluate_model_test (model=random_forrest_regr,\n",
    "                                                     X_te=self._X_test,\n",
    "                                                     y_te=self._Y_test)\n",
    "            return loss_value\n",
    "        return\n",
    "    \n",
    "    def Stacking_tuning(self, cv=5):\n",
    "        seed = 0\n",
    "        min_loss_params = {\"estimators\": None, \"final_estimator\": None, \"min_loss_value\": float('inf')}\n",
    "        \n",
    "        # Define the base estimators with their best hyperparameters\n",
    "        knn_best = KNeighborsRegressor(\n",
    "            n_neighbors=self.KNN_min_loss_params_['n_neighbor'],\n",
    "            p=self.KNN_min_loss_params_['p'],\n",
    "            weights=self.KNN_min_loss_params_['weight'],\n",
    "            algorithm=self.KNN_min_loss_params_['algorithm'],\n",
    "            leaf_size=self.KNN_min_loss_params_['leaf_size']\n",
    "        )\n",
    "\n",
    "        svm_best = SVR(\n",
    "            kernel='poly',  # Assuming 'poly' is the best kernel \n",
    "            C=self.svm_rbf_min_loss_params_['C'],\n",
    "            epsilon=self.svm_rbf_min_loss_params_['epsilon'],\n",
    "            gamma=self.svm_rbf_min_loss_params_['gamma']\n",
    "        )\n",
    "\n",
    "        ada_best = AdaBoostRegressor(\n",
    "            n_estimators=self.Ada_Boost_min_loss_params_['n_estimator'],\n",
    "            learning_rate=self.Ada_Boost_min_loss_params_['learning_rate'],\n",
    "            random_state=self.Ada_Boost_min_loss_params_['random_state']\n",
    "        )\n",
    "\n",
    "        rf_best = RandomForestRegressor(\n",
    "            n_estimators=self.random_forrest_min_loss_params_['n_estimator'],\n",
    "            max_depth=self.random_forrest_min_loss_params_['max_depth'],\n",
    "            min_samples_split=self.random_forrest_min_loss_params_['min_samples_split'],\n",
    "            min_samples_leaf=self.random_forrest_min_loss_params_['min_samples_leaf'],\n",
    "            bootstrap=self.random_forrest_min_loss_params_['bootstrap'],\n",
    "            random_state=self.random_forrest_min_loss_params_['random_state'],\n",
    "            max_features=self.random_forrest_min_loss_params_['max_features']\n",
    "        )\n",
    "\n",
    "        # List of possible final estimators\n",
    "        final_estimators = [\n",
    "            LinearRegression(),\n",
    "            SVR(),\n",
    "            KNeighborsRegressor(),\n",
    "            AdaBoostRegressor(),\n",
    "            RandomForestRegressor()\n",
    "        ]\n",
    "\n",
    "        X_tr, y_tr, X_val, y_val = self.__train_val_spliter(seed=seed)\n",
    "        \n",
    "        for final_estimator in final_estimators:\n",
    "            stacking_regressor = StackingRegressor(\n",
    "                estimators=[\n",
    "                    ('knn', knn_best),\n",
    "                    ('svm', svm_best),\n",
    "                    ('ada', ada_best),\n",
    "                    ('rf', rf_best)\n",
    "                ],\n",
    "                final_estimator=final_estimator,\n",
    "                cv=cv,\n",
    "                n_jobs=-1,\n",
    "                passthrough=True\n",
    "            )\n",
    "\n",
    "            loss = self.__evaluate_model(model=stacking_regressor, X_tr=X_tr, y_tr=y_tr, X_val=X_val, y_val=y_val)\n",
    "            \n",
    "            if loss < min_loss_params[\"min_loss_value\"]:\n",
    "                min_loss_params[\"estimators\"] = stacking_regressor.estimators\n",
    "                min_loss_params[\"final_estimator\"] = final_estimator\n",
    "                min_loss_params[\"min_loss_value\"] = loss\n",
    "\n",
    "            seed += 1\n",
    "        \n",
    "        self.Stacking_min_loss_params_ = min_loss_params\n",
    "        return\n",
    "\n",
    "    def Stacking_score (cv=5):\n",
    "        if self.Stacking_min_loss_params_[\"min_loss_value\"] != None:\n",
    "            estimators = self.Stacking_min_loss_params_[\"estimators\"]\n",
    "            final_estimator = self.Stacking_min_loss_params_[\"final_estimator\"]\n",
    "            stacking_regressor = StackingRegressor(\n",
    "                                                    estimators=estimators,\n",
    "                                                    final_estimator=final_estimator,\n",
    "                                                    cv=cv,\n",
    "                                                    n_jobs=-1,\n",
    "                                                    passthrough=True\n",
    "                                                )\n",
    "            loss_value = self.__evaluate_model_test (model=stacking_regressor,\n",
    "                                                     X_te=self._X_test,\n",
    "                                                     y_te=self._Y_test)\n",
    "            return loss_value\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759f225",
   "metadata": {},
   "source": [
    "# Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddef4f4f-7f69-4e3f-9719-160136691931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_cases                     7.500000e+01\n",
      "new_deaths                    2.000000e+00\n",
      "new_vaccinations              0.000000e+00\n",
      "new_people_vaccinated         0.000000e+00\n",
      "reproduction_rate             9.500000e-01\n",
      "stringency_index              8.796000e+01\n",
      "excess_mortality             -1.000000e+03\n",
      "population_density            4.272900e+01\n",
      "median_age                    1.960000e+01\n",
      "aged_65_older                 2.822000e+00\n",
      "aged_70_older                 1.882000e+00\n",
      "cardiovasc_death_rate         3.078460e+02\n",
      "diabetes_prevalence           1.820000e+00\n",
      "female_smokers                1.600000e+00\n",
      "male_smokers                  3.070000e+01\n",
      "hospital_beds_per_thousand    1.700000e+00\n",
      "life_expectancy               6.149000e+01\n",
      "human_development_index       5.710000e-01\n",
      "population                    1.632054e+07\n",
      "Name: 41007, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_list, Y_list, country_num = get_X_Y([\"CHN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e20e48d-c748-4e36-8445-6279799e3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_days_count = 2\n",
    "\n",
    "MT = Moldels_Tuning(X_list=X_list, Y_list=Y_list,\n",
    "                    country_num=country_num,\n",
    "                    prediction_days_count=prediction_days_count,\n",
    "                    batch_count=3,\n",
    "                    batch_length=10,\n",
    "                    val_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce949f",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "609274a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.KNN_tuning([2, 3, 5], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f4c86f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbor': 2,\n",
       " 'p': 3,\n",
       " 'weight': 'distance',\n",
       " 'algorithm': 'kd_tree',\n",
       " 'leaf_size': 30,\n",
       " 'min_loss_value': 1280.6872876774578}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT.KNN_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0542267e-6498-42da-b854-e4bc71670034",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.KNN_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e60dd",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f154d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_tuning(kernel='linear', Cs=[0.01, 0.1, 1, 10, 100],epsilons=[0.01, 0.05, 0.1, 0.2, 0.5] , gammas=[0.001, 0.01, 0.1, 1, 10], degrees=[2, 3, 4, 5], coef0s=[0, 0.1, 0.5, 1, 5])\n",
    "MT.svm_tuning(kernel='poly', Cs=[0.01, 0.1, 1, 10, 100],epsilons=[0.01, 0.05, 0.1, 0.2, 0.5] , gammas=[0.001, 0.01, 0.1, 1, 10], degrees=[2, 3, 4, 5], coef0s=[0, 0.1, 0.5, 1, 5])\n",
    "MT.svm_tuning(kernel='rbf', Cs=[0.01, 0.1, 1, 10, 100],epsilons=[0.01, 0.05, 0.1, 0.2, 0.5] , gammas=[0.001, 0.01, 0.1, 1, 10], degrees=[2, 3, 4, 5], coef0s=[0, 0.1, 0.5, 1, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_linear_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95085b6-de3d-4322-a61f-890806a4bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_score(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_poly_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61a6f9-9dc5-4530-9286-c7cd6a8c247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_score(kernel=\"poly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77107965",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_rbf_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b39fe-1ced-43dc-b07b-8484bf759b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_score(kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1b1c",
   "metadata": {},
   "source": [
    "## Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Poly_tuning(degrees=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Poly_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f221bbc-5dc3-4395-9162-0d8ea545ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Poly_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7a082",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Ada_Boost_tuning(n_estimators=[50, 100, 200, 500], learning_rates=[0.01, 0.1, 0.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Ada_Boost_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b53dc-de08-4d69-9e48-49cb41a733c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Ada_Boost_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2d25e",
   "metadata": {},
   "source": [
    "## Random-Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.random_forrest_tuning(n_estimators=[50, 100, 200], max_features=['sqrt', 'log2'], max_depths=[1,3,6], min_samples_splits=[2, 5], min_samples_leaves=[1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.random_forrest_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd094b-d4f9-4a0f-91a6-a1216f6850f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.random_forrest_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c97208",
   "metadata": {},
   "source": [
    "## stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Stacking_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Stacking_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8ed83-7e7b-4b0e-a253-e94d7f759fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Stacking_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a741e1-9743-4c58-9c3e-0a270b2978dc",
   "metadata": {},
   "source": [
    "## Testing rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cadd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if my rescaling is working as intended\n",
    "\n",
    "# Test scaling and rescaling\n",
    "sample_y = [y_scaled[0], y_scaled[10], y_scaled[100]]\n",
    "print(\"Sample scaled values:\", sample_y)\n",
    "\n",
    "rescaled_y = rescale_data(min_max_scaler_Y, sample_y)\n",
    "print(\"Rescaled values:\", rescaled_y)  \n",
    "\n",
    "original_y = [Y_list[0], Y_list[10], Y_list[100]]\n",
    "print(\"Original values:\", original_y)\n",
    "\n",
    "# Check if rescaled values match the original values\n",
    "print(\"Match:\", np.allclose(rescaled_y, original_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1114a-b461-4cbd-a8e3-feaf79287d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1]:\n",
    "    for j in [None]:\n",
    "        print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee89e7",
   "metadata": {},
   "source": [
    "## Tuning function but not working as I except."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022cca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def evaluate_and_plot_models(models, X_tr, y_tr, X_te, y_te, min_max_scaler_Y, rows_to_plot):\n",
    "    results = []\n",
    "\n",
    "    # Evaluate each model with its respective hyperparameters\n",
    "    for model_info in models:\n",
    "        model_type = model_info['type']\n",
    "        hyperparameters = model_info['params']\n",
    "        \n",
    "        for hyperparam in hyperparameters:\n",
    "            if model_type == 'RandomForest':\n",
    "                model = RandomForestRegressor(**hyperparam, random_state=0)\n",
    "            elif model_type == 'KNeighbors':\n",
    "                model = KNeighborsRegressor(**hyperparam)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported model type\")\n",
    "            \n",
    "            model_err_pred = evaluate_model(model, X_tr, y_tr, X_te, y_te, min_max_scaler_Y, model_name=model_type, monitoring=False)\n",
    "            results.append((hyperparam,) + model_err_pred)\n",
    "\n",
    "            print(f\"Process for {model_type} with {hyperparam} done.\")\n",
    "\n",
    "    # Convert the list of results to a DataFrame\n",
    "    if models[0]['type'] == 'RandomForest':\n",
    "        columns = [\"params\", \"MAE\", \"MSE\", \"y_pred\", \"y_test_rescaled\"]\n",
    "    elif models[0]['type'] == 'KNeighbors':\n",
    "        columns = [\"params\", \"MAE\", \"MSE\", \"y_pred\", \"y_test_rescaled\"]\n",
    "    \n",
    "    tuning_data_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, len(rows_to_plot), figsize=(18, 6))\n",
    "\n",
    "    for i, row in enumerate(rows_to_plot):\n",
    "        y_pred = tuning_data_df.loc[row, \"y_pred\"]\n",
    "        y_test_rescaled = tuning_data_df.loc[row, \"y_test_rescaled\"]\n",
    "        params = tuning_data_df.loc[row, \"params\"]\n",
    "        model_name = f\"{models[0]['type']} {params}\"\n",
    "        print(f\"For {params}: MAE = {tuning_data_df.loc[row, 'MAE']} , MSE = {tuning_data_df.loc[row, 'MSE']}\")\n",
    "        plot_model(y_pred, y_test_rescaled, model_name, axs[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return tuning_data_df\n",
    "\n",
    "# Define the models and hyperparameters\n",
    "models = [\n",
    "    {\n",
    "        'type': 'RandomForest',\n",
    "        'params': [{'max_depth': depth} for depth in range(2, 10)]\n",
    "    },\n",
    "    {\n",
    "        'type': 'KNeighbors',\n",
    "        'params': [{'n_neighbors': k, 'p': p} for p in [1, 2, 3] for k in [2, 5, 10, 15]]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with the models and data\n",
    "tuning_data_df = evaluate_and_plot_models(models, X_tr, y_tr, X_te, y_te, min_max_scaler_Y, rows_to_plot=[0, 4, 7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f9205f-ce74-44eb-885f-62f117f8a858",
   "metadata": {},
   "source": [
    "# Neighbors effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a316d9",
   "metadata": {},
   "source": [
    "in this part we are going to include the effect of the top 10 important neighbours of a country on the model and do the whole process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b506c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iso_code                             ZWE\n",
      "new_cases                           75.0\n",
      "new_deaths                           2.0\n",
      "new_vaccinations                     0.0\n",
      "new_people_vaccinated                0.0\n",
      "reproduction_rate                   0.95\n",
      "stringency_index                   87.96\n",
      "excess_mortality                 -1000.0\n",
      "population_density                42.729\n",
      "median_age                          19.6\n",
      "aged_65_older                      2.822\n",
      "aged_70_older                      1.882\n",
      "cardiovasc_death_rate            307.846\n",
      "diabetes_prevalence                 1.82\n",
      "female_smokers                       1.6\n",
      "male_smokers                        30.7\n",
      "hospital_beds_per_thousand           1.7\n",
      "life_expectancy                    61.49\n",
      "human_development_index            0.571\n",
      "population                    16320539.0\n",
      "Name: 41183, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "# Load datasets\n",
    "df = pd.read_csv(\"../Data/Raw/new_approach/Weekly_Covid_Data.csv\")\n",
    "interaction_matrix = pd.read_csv(\"../Data/Interaction_matrix/interaction_matrix.csv\")\n",
    "interaction_matrix.index = interaction_matrix.columns\n",
    "\n",
    "# Preprocessing\n",
    "df.drop([\"week_no\"], axis=1, inplace=True)\n",
    "pop = df.iloc[0][\"population\"]\n",
    "last_country = df.iloc[234*176 - 1]\n",
    "print(last_country)\n",
    "cnt = df[\"population\"].value_counts()\n",
    "\n",
    "# Identify top 10 correlated countries for each country, ignoring invalid values\n",
    "top_10_neighbors = {}\n",
    "valid_countries = set(interaction_matrix.index)\n",
    "\n",
    "for country in interaction_matrix.index:\n",
    "    valid_correlations = interaction_matrix.loc[country].replace([-1, 0, np.nan], np.nan).dropna()\n",
    "    if len(valid_correlations) >= 10:\n",
    "        top_10_neighbors[country] = valid_correlations.nlargest(10).index.tolist()\n",
    "    else:\n",
    "        valid_countries.remove(country)\n",
    "\n",
    "# Normalize interaction matrix for weights\n",
    "normalized_interaction_matrix = interaction_matrix.div(interaction_matrix.max(axis=1), axis=0)\n",
    "\n",
    "# Map countries to their indices in the data\n",
    "country_indices = {df.iloc[i * 176]['iso_code']: i for i in range(234) if df.iloc[i * 176]['iso_code'] in valid_countries}\n",
    "\n",
    "# Initialize feature and label lists\n",
    "prediction_days_count = 2\n",
    "features_list = []\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "# Create feature list including neighboring country features with weighted effect\n",
    "for i in range(234):  # 234 is the total number of countries\n",
    "    country = df.loc[i * 176, 'iso_code']\n",
    "    if country not in valid_countries:\n",
    "        continue  # Skip countries not in the valid set\n",
    "\n",
    "    neighbors = top_10_neighbors.get(country, [])\n",
    "    \n",
    "    for j in range(176):  # 176 is the number of weeks we have\n",
    "        row_list = df.loc[i * 176 + j, :].values[1:].flatten().tolist()\n",
    "        \n",
    "        for neighbor in neighbors:\n",
    "            if neighbor in country_indices:\n",
    "                neighbor_index = country_indices[neighbor]\n",
    "                weight = normalized_interaction_matrix.loc[country, neighbor]\n",
    "                weighted_features = df.loc[neighbor_index * 176 + j, :].values[1:].flatten() * weight\n",
    "                row_list.extend(weighted_features.tolist())\n",
    "        \n",
    "        features_list.append(row_list)\n",
    "\n",
    "# Extend the features list for prediction days\n",
    "for i in range((len(features_list) - prediction_days_count) + 1):\n",
    "    fl = features_list[i]\n",
    "    fl_len = len(features_list[i])\n",
    "    this_population = features_list[i][fl_len - 1]\n",
    "    for j in range(1, prediction_days_count):\n",
    "        if this_population == features_list[i + j][fl_len - 1]:\n",
    "            fl.extend(features_list[i + j])\n",
    "        else:\n",
    "            this_population = -1\n",
    "            break\n",
    "    if this_population != -1:\n",
    "        X_list.append(fl)\n",
    "        Y_list.append(features_list[(i + prediction_days_count) - 1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db7809-9144-429d-92de-80d5349d86f2",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    color: blue; /* Text color */\n",
    "    background-color: #f0f0f0; /* Light grey background */\n",
    "    font-family: 'Pacifico', cursive; /* Stylish font */\n",
    "    font-size: 40px; /* Font size */\n",
    "    font-weight: bold; /* Bold text */\n",
    "    padding: 20px; /* Padding around the text */\n",
    "    border: 2px solid #ccc; /* Border with a light grey color */\n",
    "    border-radius: 10px; /* Rounded corners */\n",
    "    text-align: center; /* Centered text */\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Subtle shadow */\n",
    "    position: relative; /* Required for pseudo-elements positioning */\n",
    "    display: inline-block; /* Inline-block for proper spacing */\n",
    "\">\n",
    "    <span style=\"\n",
    "        position: absolute; /* Absolute positioning */\n",
    "        left: -40px; /* Positioning arrow on the left */\n",
    "        top: 50%; /* Vertically center */\n",
    "        transform: translateY(-50%); /* Adjust vertical alignment */\n",
    "        font-size: 30px; /* Arrow size */\n",
    "        color: red; /* Arrow color */\n",
    "    \">➤</span>\n",
    "    ML Modeling\n",
    "    <span style=\"\n",
    "        position: absolute; /* Absolute positioning */\n",
    "        right: -40px; /* Positioning arrow on the right */\n",
    "        top: 50%; /* Vertically center */\n",
    "        transform: translateY(-50%); /* Adjust vertical alignment */\n",
    "        font-size: 30px; /* Arrow size */\n",
    "        color: red; /* Arrow color */\n",
    "    \">➤</span>\n",
    "</div>\n",
    "\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Pacifico&display=swap\" rel=\"stylesheet\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e697d7e",
   "metadata": {},
   "source": [
    "# Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9609a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT = Moldels_Tuning(X_list=X_list, Y_list=Y_list,\n",
    "                    prediction_days_count=prediction_days_count,\n",
    "                    batch_count=3,\n",
    "                    batch_length=10,\n",
    "                    val_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37458020",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4cead0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.KNN_tuning([2, 3, 5], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82c13b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbor': 2,\n",
       " 'p': 1,\n",
       " 'weight': 'distance',\n",
       " 'algorithm': 'ball_tree',\n",
       " 'leaf_size': 30,\n",
       " 'min_loss_value': 2059.7004364142263}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT.KNN_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912007e-4484-4892-9e05-cb8d501cde0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.KNN_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf142b5a",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "329993c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_tuning(kernel='linear', Cs=[0.01, 0.1, 1, 10, 100],epsilons=[0.01, 0.05, 0.1, 0.2, 0.5] , gammas=[0.001, 0.01, 0.1, 1, 10], degrees=[2, 3, 4, 5], coef0s=[0, 0.1, 0.5, 1, 5])\n",
    "MT.svm_tuning(kernel='poly', Cs=[0.01, 0.1, 1, 10, 100],epsilons=[0.01, 0.05, 0.1, 0.2, 0.5] , gammas=[0.001, 0.01, 0.1, 1, 10], degrees=[2, 3, 4, 5], coef0s=[0, 0.1, 0.5, 1, 5])\n",
    "MT.svm_tuning(kernel='rbf', Cs=[0.01, 0.1, 1, 10, 100],epsilons=[0.01, 0.05, 0.1, 0.2, 0.5] , gammas=[0.001, 0.01, 0.1, 1, 10], degrees=[2, 3, 4, 5], coef0s=[0, 0.1, 0.5, 1, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c1d8105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100,\n",
       " 'epsilon': 0.01,\n",
       " 'gamma': 0.001,\n",
       " 'degree': 4,\n",
       " 'coef0': 0.1,\n",
       " 'min_loss_value': 16908.20125623387}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT.svm_linear_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaae97f-25cb-4673-a89a-d084f80b5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_score(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e161aa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1,\n",
       " 'epsilon': 0.01,\n",
       " 'gamma': 0.1,\n",
       " 'degree': 3,\n",
       " 'coef0': 1,\n",
       " 'min_loss_value': 17181.352220728666}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT.svm_poly_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d047e4-1127-4444-8282-4a9bc15f5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_score(kernel=\"poly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "89c7fb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10,\n",
       " 'epsilon': 0.01,\n",
       " 'gamma': 0.01,\n",
       " 'degree': 4,\n",
       " 'coef0': 1,\n",
       " 'min_loss_value': 20223.80328962984}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT.svm_rbf_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d317808-3370-43ec-a22c-6bb8204bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.svm_score(kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62760ec",
   "metadata": {},
   "source": [
    "## Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "098d3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Poly_tuning(degrees=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b8d35494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degree': 1,\n",
       " 'interaction_only': True,\n",
       " 'include_bias': True,\n",
       " 'min_loss_value': 2289.267098941311}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT.Poly_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d33504-09ae-4631-9e5d-64deeb88bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Poly_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e86fae",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "21e81f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Ada_Boost_tuning(n_estimators=[50, 100, 200, 500], learning_rates=[0.01, 0.1, 0.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "61b8e295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimator': 50,\n",
       " 'learning_rate': 0.1,\n",
       " 'random_state': 0,\n",
       " 'estimator': None,\n",
       " 'min_loss_value': 3842.6826828266435}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT.Ada_Boost_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566642f1-df03-4143-a30f-f9787f137644",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Ada_Boost_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d65f6",
   "metadata": {},
   "source": [
    "## Random-Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "987826b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.random_forrest_tuning(n_estimators=[50, 100, 200], max_features=['sqrt', 'log2'], max_depths=[1,3,6], min_samples_splits=[2, 5], min_samples_leaves=[1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2cce3d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimator': 200,\n",
       " 'max_depth': 6,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'bootstrap': False,\n",
       " 'random_state': 0,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_loss_value': 3855.443172052244}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT.random_forrest_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b3fd8-7b69-41c0-98c6-530d3f8da573",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.random_forrest_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72707a30",
   "metadata": {},
   "source": [
    "## stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b9a7d15",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "MT.Stacking_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789114a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimators': [('knn',\n",
       "   KNeighborsRegressor(algorithm='brute', n_neighbors=3, weights='distance')),\n",
       "  ('svm', SVR(C=100, epsilon=0.01, gamma=0.01, kernel='poly')),\n",
       "  ('ada', AdaBoostRegressor(learning_rate=0.01, random_state=0)),\n",
       "  ('rf',\n",
       "   RandomForestRegressor(bootstrap=False, max_depth=6, max_features='sqrt',\n",
       "                         n_estimators=200, random_state=0))],\n",
       " 'final_estimator': LinearRegression(),\n",
       " 'min_loss_value': 4.0064016728198946e-10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MT.Stacking_min_loss_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f96453-e692-4183-9f3d-eb81a7902b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT.Stacking_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e066eb",
   "metadata": {},
   "source": [
    "## Testing rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5799e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample scaled values: [array([0.]), array([4.20007403e-07]), array([4.44713721e-06])]\n",
      "Rescaled values: [  0.  17. 180.]\n",
      "Original values: [0.0, 17.0, 180.0]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "#check to see if my rescaling is working as intended\n",
    "\n",
    "# Test scaling and rescaling\n",
    "sample_y = [y_scaled[0], y_scaled[10], y_scaled[100]]\n",
    "print(\"Sample scaled values:\", sample_y)\n",
    "\n",
    "rescaled_y = rescale_data(min_max_scaler_Y, sample_y)\n",
    "print(\"Rescaled values:\", rescaled_y)  \n",
    "\n",
    "original_y = [Y_list[0], Y_list[10], Y_list[100]]\n",
    "print(\"Original values:\", original_y)\n",
    "\n",
    "# Check if rescaled values match the original values\n",
    "print(\"Match:\", np.allclose(rescaled_y, original_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2063a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None\n"
     ]
    }
   ],
   "source": [
    "for i in [1]:\n",
    "    for j in [None]:\n",
    "        print(i, j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
